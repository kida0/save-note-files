{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOZ3Tdrw62EgC8cve8DFaPh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "gMlMrXJdOPrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession, Row, Column\n",
        "from pyspark.sql import functions as f\n",
        "from pyspark.sql.functions import pandas_udf, expr\n",
        "import pyspark.pandas as ps\n",
        "\n",
        "import os, sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import easydict\n",
        "\n",
        "from typing import *\n",
        "from tqdm import tqdm\n",
        "\n",
        "os.environ['PYARROW_IGNORE_TIMEZONE'] = '1'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jxsr1kxgHGUB",
        "outputId": "c4a77c40-f62a-40f8-86ee-b326b5b695a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pyspark/pandas/__init__.py:49: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __PySpark__\n",
        "\n",
        "|Numpy|PySpark|\n",
        "|---|---|\n",
        "|np.int8|ByteType|\n",
        "|np.int16|ShortType|\n",
        "|np.int32|IntegerType|\n",
        "|np.int64|LongType|\n",
        "|np.float32|FloatType|\n",
        "|np.float64|DoubleType|\n",
        "|np.str|StringType|"
      ],
      "metadata": {
        "id": "wPLIlpzeeYAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame을 다루기위한 SparkSession 생성\n",
        "spark = (SparkSession\n",
        "         .builder\n",
        "         .master('local')\n",
        "         .appName('Mata')\n",
        "         .config(conf=SparkConf())\n",
        "         .getOrCreate())\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "UFSitXVgeZWj",
        "outputId": "6742b88c-1839-4d5b-fb7e-6a632dea7ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f10d9a70670>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://f44b98b218e9:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Mata</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data load, save"
      ],
      "metadata": {
        "id": "tCPCMZjLG2sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.DataFrame()               # pandas.core.frame.DataFrame\n",
        "# pdf = spark.createDataFrame(df)   # pyspark.sql.dataframe.DataFrame\n",
        "# psdf = ps.from_pandas(df)         # pyspark.pandas.frame.DataFrame\n",
        "# pdf = psdf.to_spark()\n",
        "# psdf = pdf.pandas_api()\n",
        "# df = pdf.toPandas()\n",
        "# df = psdf.to_pandas()\n",
        "pdf.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kn5ICqU77QoN",
        "outputId": "177d318d-7411-4cff-fd2a-2a6183fec1f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- a: long (nullable = true)\n",
            " |-- b: double (nullable = true)\n",
            " |-- c: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = (spark\n",
        "      .read\n",
        "      .option('inferSchema', True)\n",
        "      .option('header', True)\n",
        "      .csv('/raw_dataset.csv')\n",
        "      .limit(10_000)))"
      ],
      "metadata": {
        "id": "HrsLW-pK_ZT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf = spark.read.csv('foo.csv', header=True, inferSchema=True)\n",
        "pdf.write.csv('foo.csv', header=True)\n",
        "\n",
        "pdf = spark.read.parquet('bar.parquet')\n",
        "pdf.write.parquet('bar.parquet')\n",
        "\n",
        "psdf = ps.read_csv('foo.csv')\n",
        "psdf.to_csv('foo.csv')\n",
        "\n",
        "psdf = ps.read_parquet('bar.parquet')\n",
        "psdf.to_parquet('bar.parquet')"
      ],
      "metadata": {
        "id": "uX484DWq9aZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "json_dict = [\n",
        "    {'age': None, 'name': 'Michael'},\n",
        "    {'age': 30, 'name': 'Andy'},\n",
        "    {'age': 19, 'name': 'Justin'},\n",
        "]\n",
        "\n",
        "with open('info.json', 'w') as json_file:\n",
        "    json.dump(json_dict, json_file)\n",
        "\n",
        "pdf = spark.read.json('info.json')\n",
        "psdf = ps.read_json('info.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRBphh75_txT",
        "outputId": "648caa60-c9b7-4013-daeb-0e57397d7226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark SQL\n",
        "`SQL`을 Spark 함수로 정의"
      ],
      "metadata": {
        "id": "D625Mrbg5W0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CoreClasses"
      ],
      "metadata": {
        "id": "W5F5FijBjKmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# withColumn\n",
        "# withColumn(colname, col): 새로운 column 추가\n",
        "pdf = pdf.withColumn('new_a', F.abs(pdf.a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGGiDvJg79-1",
        "outputId": "c13b6b3f-76fd-424e-bc5d-ddf246f44e81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[a: bigint, b: double, c: string, new_a: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cast, alias, asc, desc\n",
        "# cast(type): 타입 변경, alias(colname): 변수명 변경\n",
        "(1 / raw['q_seq']).cast('double').asc().alias('sorted_q_seq')\n",
        "df.select(col('q_seq').alias('concept_count'), col('rightFlag'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGUYtS2wfkNt",
        "outputId": "d59f8c30-d68e-4be7-c0e1-b4f2b6423ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Column<'(1 / conceptCount) ASC NULLS FIRST AS sorted_conceptCount'>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 변수명 변경\n",
        "new_colnames = []\n",
        "df = df.toDF(*new_colnames)\n",
        "\n",
        "# 부분 변수명 변경\n",
        "df = df.withColumnRenamed('old_colname', 'new_colname')"
      ],
      "metadata": {
        "id": "9NDMflZm5-9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select, filter, between, isin, contains, isNull, isNotNull\n",
        "# select: 조건에 맞는 열 선택, filter: 조건에 맞는 행 선택\n",
        "raw.select(raw.q_seq, raw.q_seq.between(10, 12))         # 10~12 리턴\n",
        "raw.select(raw.q_seq, raw.q_seq.isin([10, 11, 12]))      # 10~12 리턴\n",
        "raw.select(raw.q_seq, raw.q_seq.contains('c'))           # 'c'가 포함되면 리턴"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vulySmpkfkYm",
        "outputId": "d28210e5-676f-47cf-e1ec-11a82e9cda5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[q_seq: int, ((conceptCount >= 10) AND (conceptCount <= 12)): boolean]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# like, startswith, substr\n",
        "raw.filter(raw.ev_date.like('2022-10%'))\n",
        "raw.filter(raw.ev_date.startswith('2022-10'))\n",
        "raw.select(raw.ev_date.substr(1, 4)).alias('year')"
      ],
      "metadata": {
        "id": "clsNw19ShbXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# when\n",
        "# select와 조건 함수 when을 이용한 변수 생성 (=SELECT, CASE WHEN)\n",
        "raw.select(raw.expHeight, F.when(raw.expHeight > 2000, 0).when(raw.expHeight < 1000, 1).otherwise(0)).limit(3).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gIq8jeRiJZn",
        "outputId": "c8dd57ce-e27c-4faa-a9a1-3d062d1384f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------------------------------------------------------------------------+\n",
            "|expHeight|CASE WHEN (expHeight > 2000) THEN 0 WHEN (expHeight < 1000) THEN 1 ELSE 0 END|\n",
            "+---------+-----------------------------------------------------------------------------+\n",
            "|      100|                                                                            1|\n",
            "|      116|                                                                            1|\n",
            "|      192|                                                                            1|\n",
            "+---------+-----------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataFrame"
      ],
      "metadata": {
        "id": "s92RIeKajMtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# collect, dtypes\n",
        "# collect(): 데이터를 Row 형태로 리턴\n",
        "raw.collect()\n",
        "raw.dtypes\n",
        "raw.schema"
      ],
      "metadata": {
        "id": "Cgo3H9RZjOAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop, drop_duplicates\n",
        "raw.drop('q_seq')\n",
        "raw.drop_duplicates(['ex_seq', 'q_seq'])"
      ],
      "metadata": {
        "id": "SKYo9QUCjOnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropna, na.drop, fillna, na.fill\n",
        "raw.dropna(how='any')   # how='all': 모두 nan인 경우만 drop\n",
        "raw.na.drop()\n",
        "raw.fillna(0)\n",
        "raw.na.fill(0)\n",
        "raw.na.fill({'rightFlag': 0, 'expHeight': 1000})"
      ],
      "metadata": {
        "id": "-ndsvi67jOu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter, select\n",
        "# raw.filter(): 조건에 맞는 행 선택\n",
        "raw.filter(raw.expHeight > 0)\n",
        "raw.filter('expHeight > 0')\n",
        "\n",
        "# raw.select(): 조건에 투영하여(project) 새로운 DataFrame 리턴\n",
        "raw.select('*').collect()\n",
        "raw.select('ex_seq', ('q_seq' + 10).alias('new_q'))"
      ],
      "metadata": {
        "id": "ZWVzUg9sjO2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### __transform__"
      ],
      "metadata": {
        "id": "93j77UcWMAA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transform\n",
        "# transform(func, *args, *kwargs): df를 리턴하는 func을 받아 새로운 DataFrame 생성\n",
        "def cast_all_to_int(input_df):\n",
        "    return input_df.select([F.col(col_name).cast('int') for col_name in input_df.columns])\n",
        "\n",
        "def sort_columns_asc(input_df):\n",
        "    return input_df.select(*sorted(input_df.columns))\n",
        "\n",
        "df = spark.createDataFrame([(1, 1.0), (2, 2.0)], ['int', 'float'])\n",
        "df.transform(cast_all_to_int).transform(sort_columns_asc).show()"
      ],
      "metadata": {
        "id": "Em-rZF3tLZpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# join\n",
        "raw.join(raw.rightFlag, on='seqNo', how='inner').select('ex_seq', 'q_seq').sort(F.desc('q_seq'))"
      ],
      "metadata": {
        "id": "fwg1XW-FoXdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show, limit, first, head, tail, last\n",
        "raw.show()\n",
        "raw.limit()\n",
        "raw.first()\n",
        "raw.head()\n",
        "raw.tail()"
      ],
      "metadata": {
        "id": "dXVccTZinmCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sort\n",
        "raw.sort('seqNo', ascending=False)"
      ],
      "metadata": {
        "id": "Layaxi23pOTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# describe, summary\n",
        "# summary: describe에 percentile 추가됨 - summary val. 선택도 가능\n",
        "raw.describe(['expHeight']).show()\n",
        "raw.select('expHeight').summary().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIa6dRG7jOGq",
        "outputId": "6b503b20-d008-48a9-b452-05bc2c352124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+\n",
            "|summary|         expHeight|\n",
            "+-------+------------------+\n",
            "|  count|           6366626|\n",
            "|   mean| 393.4629398679929|\n",
            "| stddev|357.69690700290295|\n",
            "|    min|                -2|\n",
            "|    max|              5214|\n",
            "+-------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Groupping"
      ],
      "metadata": {
        "id": "QvMMh8VL0C0q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`pandas_udf` pandas user defined function: transfer data and Pandas to work with the data, which `allows vectorized operations`\n",
        "\n",
        "`Pandas UDF` behaves as a regular `PySpark function API` in general\n",
        "\n",
        "* `apply`는 `@pandas_udf(input_type)`을 함수위에 설정해야 함\n",
        "* `applyInPandas`는 @pandas_udf(input_type)을 설정할 필요 없지만 `schema`를 설정해야함\n",
        "* `spark.pandas`를 사용하는 경우 udf나 schema를 설정할 필요 없음"
      ],
      "metadata": {
        "id": "4RhpMys8u-19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# groupby, agg + count, mean, avg, min, max, sum\n",
        "raw.groupby(['qType', 'imageAudit']).count().collect()\n",
        "raw.groupby('qType').agg({'expHeight': 'mean', 'rightFlag': 'sum'})\n",
        "raw.groupby(raw.qType).agg({'*': 'count'})"
      ],
      "metadata": {
        "id": "XOn0V5neKvCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@pandas_udf('long')  # 사용하는 데이터의 input type을 명시, 복수의 경우 'col1 type1, col2 type2'\n",
        "def mean_udf(v: pd.Series) -> float:\n",
        "    return v.mean()\n",
        "\n",
        "df = spark.createDataFrame(\n",
        "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], schema=(\"id\", \"v\"))\n",
        "df.groupby('id').agg(mean_udf(df['v']).alias('mean_v')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcPKVNhjw07a",
        "outputId": "f893e52c-af94-44ae-e270-599782f657bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id|mean_v|\n",
            "+---+------+\n",
            "|  1|     1|\n",
            "|  2|     6|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(pdf):\n",
        "    v = pdf.v\n",
        "    return pdf.assign(v=(v-v.mean()) / v.std())\n",
        "\n",
        "df.groupby('id').applyInPandas(\n",
        "    normalize, schema='id long, v double').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6nGcMfMyjae",
        "outputId": "4f4a0cba-d65f-4c55-a50a-6e88e9fd95c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------+\n",
            "| id|                  v|\n",
            "+---+-------------------+\n",
            "|  1|-0.7071067811865475|\n",
            "|  1| 0.7071067811865475|\n",
            "|  2|-0.8320502943378437|\n",
            "|  2|-0.2773500981126146|\n",
            "|  2| 1.1094003924504583|\n",
            "+---+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@pandas_udf('first string, last string')\n",
        "def split_expand(s: pd.Series) -> pd.DataFrame:\n",
        "    return s.str.split(expand=True)\n",
        "\n",
        "df = spark.createDataFrame([('John Doe',)], ('name',))\n",
        "df.select(split_expand('name')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba0Rrm_iwf0e",
        "outputId": "a284720f-91d4-44b7-f083-28176203fa84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+\n",
            "|split_expand(name)|\n",
            "+------------------+\n",
            "|       {John, Doe}|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions\n",
        "\n",
        "df.isna().count()와 같이 사용하던 대부분의 `method`들을 `함수형`으로 바꿔놓음"
      ],
      "metadata": {
        "id": "yZIDZxic0Gxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# when, isnan, isnull, abs, sqrt\n",
        "# .select() 내에서 변수를 생성할 때 많이 사용함"
      ],
      "metadata": {
        "id": "aCXqy64DJgwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
        "df.select(F.isnan('a').alias('r1'), F.isnan(df.a).alias('r2')).collect()\n",
        "df.select(F.isnull('a').alias('r1'), F.isnull(df.a).alias('r2')).collect()\n",
        "df.select(F.abs('a').alias('abs_a'), F.sqrt('b').alias('sqrt_b')).show()\n",
        "\n",
        "raw.select(raw.expHeight, F.when(raw.expHeight > 2000, 0).when(raw.expHeight < 1000, 1).otherwise(0)).limit(3).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lx1rhAfd0HqZ",
        "outputId": "440e35cb-bc67-4d36-e07d-4c796695ed40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(r1=False, r2=False), Row(r1=False, r2=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transform\n",
        "df = spark.createDataFrame([(1, [1, 2, 3, 4]), (2, [1, 3, 5, 7])], (\"key\", \"values\"))\n",
        "df.select('key', F.transform('values', lambda x: x*2).alias('doubled')).show()\n",
        "\n",
        "def alternate(x, i):\n",
        "    return F.when(i%2 == 0, x).otherwise(-x)\n",
        "df.select(F.transform('values', alternate).alias('alternated')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyRRRc_K1l9j",
        "outputId": "e6727fab-587f-48c7-e2d3-52b2e821bce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------+\n",
            "|key|       doubled|\n",
            "+---+--------------+\n",
            "|  1|  [2, 4, 6, 8]|\n",
            "|  2|[2, 6, 10, 14]|\n",
            "+---+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example"
      ],
      "metadata": {
        "id": "ZEhIRiiu5EWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\n",
        "    [count(\n",
        "        when(\n",
        "            col(c).contains('None') | col(c).contains('NULL') | col(c) == '' | isnan(c), c\n",
        "        )\n",
        "    ).alias(c) for c in df.columns])"
      ],
      "metadata": {
        "id": "Lb7gnDUF5JHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(df\n",
        " .withColumn('new_col', when(df.col == 'M', 'Male')\n",
        " .when(df.col == 'F', 'Female')\n",
        " .when(df.col.isnull(), \"\") # isNull()\n",
        " .otherwise(df.gender)))"
      ],
      "metadata": {
        "id": "eWZTdqjS6VZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rK3q4tmR6uWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pandas API\n",
        "`pandas`의 함수 대부분을 사용하여 PySpark를 사용할 수 있음"
      ],
      "metadata": {
        "id": "flN6FG6t5pE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "psdf = ps.from_pandas(df)\n",
        "psdf = pdf.pandas_api(pdf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDRWlMju50Wi",
        "outputId": "2868341d-123e-4128-8563-44b56aca20ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with SQL"
      ],
      "metadata": {
        "id": "OWPfUH2y9vgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SQL문법으로 사용할 수 있는 테이블 생성\n",
        "df.createOrReplaceTempView('table')\n",
        "\n",
        "# 저장된 테이블 확인\n",
        "print(spark.catalog.listTables())"
      ],
      "metadata": {
        "id": "F5lk0uK94trp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView('table')\n",
        "spark.sql('SELECT count(*) from table').show()"
      ],
      "metadata": {
        "id": "_ryupJw59yTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@pandas_udf('integer')\n",
        "def add_one(s: pd.Series) -> pd.Series:\n",
        "    return s+1\n",
        "\n",
        "spark.udf.register('add_one', add_one)\n",
        "spark.sql('SELECT add_one(v1) FROM table').show()"
      ],
      "metadata": {
        "id": "krgXp6vz97SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark ML"
      ],
      "metadata": {
        "id": "IDvHtzgq7F72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.stat import Correlation\n",
        "from pyspark.ml.recommendation import RegressionEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import MulticlassClassification"
      ],
      "metadata": {
        "id": "-oz039_17IEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VectorAssembler: 데이터를 하나의 열로 압축 (ex) col1, col2, col3 -> [col1, col2, col3]\n",
        "required_features = ['aid', 'rank']\n",
        "assembler = VectorAssembler(inputCols=required_features, outputCol='features')\n",
        "data = assembler.transform(df)"
      ],
      "metadata": {
        "id": "Qjedqc1Z8UV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits = data.randomSplit([0.8, 0.2])\n",
        "train = splits[0]\n",
        "test = splits[1].withColumn('label', 'trueLabel')\n",
        "\n",
        "train_rows = train.count()\n",
        "test_rows = test.count()\n",
        "print('Training Rows:', train_rows, 'Test Rows:', test_rows)\n",
        "train.select('features').show(truncate=False)"
      ],
      "metadata": {
        "id": "LrMSeXy08rQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(labelCol='target', featurecols='features', maxiter=10, regParam=0.3)\n",
        "rf = RandomForestClassifier(labelCol='target', featurecols='features', maxDepth=5)"
      ],
      "metadata": {
        "id": "UVIzVuXM8-LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = rf.fit(train)\n",
        "pred_train = model.transform(train)\n",
        "pred_test = model.transform(test)\n",
        "pred_test.select('prediction').show()"
      ],
      "metadata": {
        "id": "75JLJx-E9OQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol='quality',\n",
        "    predictionCol='prediction',\n",
        "    metricNmae='accuracy'\n",
        ")\n",
        "\n",
        "acc = evaluator.evaluate(pred_train)\n",
        "print('Train accuracy = ', acc)\n",
        "\n",
        "acc = evaluator.evaluate(pred_test)\n",
        "print('Test accuracy = ', acc)"
      ],
      "metadata": {
        "id": "0Q87NdgW9l2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.stat import Correlation\n",
        "\n",
        "corr_mat = Correlation.corr(data, 'corr_vars').collect()[0][0].toArray().tolist()\n",
        "corr_mat_df = spark.createDataFrame(corr_mat, schema=df.columns)\n",
        "corr_mat_df.show()"
      ],
      "metadata": {
        "id": "_rGoQ0AZ-hdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_corr = corr_mat_df.toPandas()\n",
        "plot_corr.index = corr_mat_df.columns\n",
        "plot_corr.stype.background_gradient(cmap='Blues')"
      ],
      "metadata": {
        "id": "J_CiPtXN_ENm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Window"
      ],
      "metadata": {
        "id": "UnHLH6Mk2csO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Window\n",
        "from pyspark.sql import Row"
      ],
      "metadata": {
        "id": "_d6eIdgMyxSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "vcVD8NaI27VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq = [\n",
        "    (1,\"chip\",1),\n",
        "    (2,\"drink\",2),\n",
        "    (3,\"chip\",2),\n",
        "    (4,\"fish\",1),\n",
        "    (5,\"drink\",3),\n",
        "    (6,\"other\",5),\n",
        "    (7,\"drink\",1),\n",
        "    (8,\"fish\",4),\n",
        "    (9,\"other\",1),\n",
        "    (10,\"other\",6),\n",
        "    (11,\"drink\",5),\n",
        "    (12,\"fish\",7)\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(seq).toDF(\"id\", \"product\", \"number\")"
      ],
      "metadata": {
        "id": "O6q6bm4g25K2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Window.unboundedPreceding: 가장 작은 값의 row부터\n",
        "# Window.currentRow: 현재까지 -> 누적\n",
        "window1 = Window.partitionBy('product')\n",
        "window2 = Window.partitionBy('product').orderBy('id').rowsBetween(Window.unboundedPreceding, Window.currentRow)"
      ],
      "metadata": {
        "id": "iZeDO7Pc3VPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(df\n",
        " .withColumn('min', f.min('number').over(window1))\n",
        " .withColumn('max', f.max('number').over(window1))\n",
        " .withColumn('avg', f.avg('number').over(window1))\n",
        " .withColumn('cumulativeSum', f.sum('number').over(window2))\n",
        " .withColumn('cumulativeSum', f.sum('number').over(window2))\n",
        " .show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAwMNAPp31Dn",
        "outputId": "1fc753b7-cbb3-46d5-c483-0ebb5ecbe778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+---+---+----+-------------+\n",
            "| id|product|number|min|max| avg|cumulativeSum|\n",
            "+---+-------+------+---+---+----+-------------+\n",
            "|  1|   chip|     1|  1|  2| 1.5|            1|\n",
            "|  3|   chip|     2|  1|  2| 1.5|            3|\n",
            "|  2|  drink|     2|  1|  5|2.75|            2|\n",
            "|  5|  drink|     3|  1|  5|2.75|            5|\n",
            "|  7|  drink|     1|  1|  5|2.75|            6|\n",
            "| 11|  drink|     5|  1|  5|2.75|           11|\n",
            "|  4|   fish|     1|  1|  7| 4.0|            1|\n",
            "|  8|   fish|     4|  1|  7| 4.0|            5|\n",
            "| 12|   fish|     7|  1|  7| 4.0|           12|\n",
            "|  6|  other|     5|  1|  6| 4.0|            5|\n",
            "|  9|  other|     1|  1|  6| 4.0|            6|\n",
            "| 10|  other|     6|  1|  6| 4.0|           12|\n",
            "+---+-------+------+---+---+----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window = Window.orderBy('id')\n",
        "\n",
        "(df\n",
        " .withColumn('lead3', f.lead('number', 3, 0).over(window))\n",
        " .withColumn('lead2', f.lead('number', 2, 0).over(window))\n",
        " .withColumn('lead1', f.lead('number', 1, 0).over(window))\n",
        " .withColumn('center', f.col('number'))\n",
        " .withColumn('lag1', f.lag('number', 1, 0).over(window))\n",
        " .withColumn('lag2', f.lag('number', 2, 0).over(window))\n",
        " .withColumn('lag3', f.lag('number', 3, 0).over(window))\n",
        " .show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0oxtyJA4GAB",
        "outputId": "c70f77d4-0e3f-4d01-c15d-9ab094c95262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+-----+-----+-----+------+----+----+----+\n",
            "| id|product|number|lead3|lead2|lead1|center|lag1|lag2|lag3|\n",
            "+---+-------+------+-----+-----+-----+------+----+----+----+\n",
            "|  1|   chip|     1|    1|    2|    2|     1|   0|   0|   0|\n",
            "|  2|  drink|     2|    3|    1|    2|     2|   1|   0|   0|\n",
            "|  3|   chip|     2|    5|    3|    1|     2|   2|   1|   0|\n",
            "|  4|   fish|     1|    1|    5|    3|     1|   2|   2|   1|\n",
            "|  5|  drink|     3|    4|    1|    5|     3|   1|   2|   2|\n",
            "|  6|  other|     5|    1|    4|    1|     5|   3|   1|   2|\n",
            "|  7|  drink|     1|    6|    1|    4|     1|   5|   3|   1|\n",
            "|  8|   fish|     4|    5|    6|    1|     4|   1|   5|   3|\n",
            "|  9|  other|     1|    7|    5|    6|     1|   4|   1|   5|\n",
            "| 10|  other|     6|    0|    7|    5|     6|   1|   4|   1|\n",
            "| 11|  drink|     5|    0|    0|    7|     5|   6|   1|   4|\n",
            "| 12|   fish|     7|    0|    0|    0|     7|   5|   6|   1|\n",
            "+---+-------+------+-----+-----+-----+------+----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window = Window.orderBy('product')\n",
        "\n",
        "(df\n",
        " .withColumn('row_number', f.row_number().over(window)) # row_number(): 순서대로 숫자 맵핑\n",
        " .withColumn('rank', f.rank().over(window))             # rank: row_number()에 맞게 증가\n",
        " .withColumn('dense_rank', f.dense_rank().over(window)) # dense_rank(): window에 따른 순서로 증가\n",
        " .show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FWj78KV58pd",
        "outputId": "5c943e4d-12c3-40e2-d214-b87e3532dfe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+----------+----+----------+\n",
            "| id|product|number|row_number|rank|dense_rank|\n",
            "+---+-------+------+----------+----+----------+\n",
            "|  1|   chip|     1|         1|   1|         1|\n",
            "|  3|   chip|     2|         2|   1|         1|\n",
            "|  2|  drink|     2|         3|   3|         2|\n",
            "|  5|  drink|     3|         4|   3|         2|\n",
            "|  7|  drink|     1|         5|   3|         2|\n",
            "| 11|  drink|     5|         6|   3|         2|\n",
            "|  4|   fish|     1|         7|   7|         3|\n",
            "|  8|   fish|     4|         8|   7|         3|\n",
            "| 12|   fish|     7|         9|   7|         3|\n",
            "|  6|  other|     5|        10|  10|         4|\n",
            "|  9|  other|     1|        11|  10|         4|\n",
            "| 10|  other|     6|        12|  10|         4|\n",
            "+---+-------+------+----------+----+----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}